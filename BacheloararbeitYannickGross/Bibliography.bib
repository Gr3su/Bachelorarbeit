@article{timeSeriesDataMining,
  author     = {Esling, Philippe and Agon, Carlos},
  title      = {Time-series data mining},
  year       = {2012},
  issue_date = {November 2012},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {45},
  number     = {1},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/2379776.2379788},
  doi        = {10.1145/2379776.2379788},
  abstract   = {In almost every scientific field, measurements are performed over time. These observations lead to a collection of organized data called time series. The purpose of time-series data mining is to try to extract all meaningful knowledge from the shape of data. Even if humans have a natural capacity to perform these tasks, it remains a complex problem for computers. In this article we intend to provide a survey of the techniques applied for time-series data mining. The first part is devoted to an overview of the tasks that have captured most of the interest of researchers. Considering that in most cases, time-series task relies on the same components for implementation, we divide the literature depending on these common aspects, namely representation techniques, distance measures, and indexing methods. The study of the relevant literature has been categorized for each individual aspects. Four types of robustness could then be formalized and any kind of distance could then be classified. Finally, the study submits various research trends and avenues that can be explored in the near future. We hope that this article can provide a broad and deep understanding of the time-series data mining research field.},
  journal    = {ACM Comput. Surv.},
  month      = dec,
  articleno  = {12},
  numpages   = {34},
  keywords   = {time series, temporal analysis, stream analysis, similarity measures, sequence matching, query by content, data mining, data indexing, Distance measures}
}

@article{compressionSurvey,
  title     = {Time series compression survey},
  author    = {Chiarot, Giacomo and Silvestri, Claudio},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {10},
  pages     = {1--32},
  year      = {2023},
  publisher = {ACM New York, NY}
}

@article{reviewOutlierDetection,
  author     = {Bl\'{a}zquez-Garc\'{\i}a, Ane and Conde, Angel and Mori, Usue and Lozano, Jose A.},
  title      = {A Review on Outlier/Anomaly Detection in Time Series Data},
  year       = {2021},
  issue_date = {April 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {54},
  number     = {3},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3444690},
  doi        = {10.1145/3444690},
  abstract   = {Recent advances in technology have brought major breakthroughs in data collection, enabling a large amount of data to be gathered over time and thus generating time series. Mining this data has become an important task for researchers and practitioners in the past few years, including the detection of outliers or anomalies that may represent errors or events of interest. This review aims to provide a structured and comprehensive state-of-the-art on unsupervised outlier detection techniques in the context of time series. To this end, a taxonomy is presented based on the main aspects that characterize an outlier detection technique.},
  journal    = {ACM Comput. Surv.},
  month      = apr,
  articleno  = {56},
  numpages   = {33},
  keywords   = {Outlier detection, anomaly detection, data mining, software, taxonomy, time series}
}

@article{pyod,
  author  = {Chen, Sihan and Qian, Zhuangzhuang and Siu, Wingchun and Hu, Xingcan and Li, Jiaqi and Li, Shawn and Qin, Yuehan and Yang, Tiankai and Xiao, Zhuo and Ye, Wanghao and Zhang, Yichi and Dong, Yushun and Zhao, Yue},
  title   = {PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection},
  journal = {arXiv preprint arXiv:2412.12154},
  year    = {2024}
}

@article{numpy,
  title     = {Array programming with {NumPy}},
  author    = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
               van der Walt and Ralf Gommers and Pauli Virtanen and David
               Cournapeau and Eric Wieser and Julian Taylor and Sebastian
               Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
               and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
               Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
               R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
               G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
               Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
               Travis E. Oliphant},
  year      = {2020},
  month     = sep,
  journal   = {Nature},
  volume    = {585},
  number    = {7825},
  pages     = {357--362},
  doi       = {10.1038/s41586-020-2649-2},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1038/s41586-020-2649-2}
}

@article{faissAehnlichkeitssuche,
  title     = {Billion-scale similarity search with {GPUs}},
  author    = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal   = {IEEE Transactions on Big Data},
  volume    = {7},
  number    = {3},
  pages     = {535--547},
  year      = {2019},
  publisher = {IEEE}
}

@book{dataCompressionSalmon,
  author    = {Salomon, David},
  title     = {Data Compression: The Complete Reference},
  edition   = {3},
  publisher = {Springer},
  address   = {New York},
  year      = {2004},
  isbn      = {978-0-387-40697-8},
  doi       = {10.1007/b97635}
}

@misc{meinGithubCode,
  author       = {Groß, Yannick},
  title        = {Bachelorarbeit},
  year         = {2025},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/Gr3su/Bachelorarbeit}}
}

@misc{leastSquares,
  author       = {Weisstein, Eric W.},
  title        = {Least Squares Fitting},
  howpublished = {\url{https://mathworld.wolfram.com/LeastSquaresFitting.html}},
  note         = {From MathWorld -- A Wolfram Resource},
  year         = {n.d.},
  urldate      = {2025-07-29}
}

@article{pyWavelets,
  doi       = {10.21105/joss.01237},
  url       = {https://doi.org/10.21105/joss.01237},
  year      = {2019},
  publisher = {The Open Journal},
  volume    = {4},
  number    = {36},
  pages     = {1237},
  author    = {Lee, Gregory R. and Gommers, Ralf and Waselewski, Filip and Wohlfahrt, Kai and O'Leary, Aaron},
  title     = {PyWavelets: A Python package for wavelet analysis},
  journal   = {Journal of Open Source Software}
}

@article{ecadWetterdaten,
  author    = {Klein Tank, A.M.G. and Coauthors},
  title     = {Daily dataset of 20th-century surface air temperature and precipitation series for the European Climate Assessment},
  journal   = {International Journal of Climatology},
  volume    = {22},
  number    = {12},
  pages     = {1441--1453},
  year      = {2002},
  doi       = {10.1002/joc.773},
  url       = {https://www.ecad.eu},
  note      = {Vor allem: \url{https://www.ecad.eu/dailydata/predefinedseries.php}}
}

@misc{nvidiaStock,
  author       = {Shamim, Adil},
  title        = {NVIDIA Stock Market History [Dataset]},
  howpublished = {Verfügbar auf Kaggle unter \url{https://www.kaggle.com/datasets/adilshamim8/nvidia-stock-market-history}},
  year         = {2025},
  note         = {Letzer Zugriff: 2025‑06‑11}
}

@misc{ecg500,
  author       = {Y. Chen and E. Keogh},
  title        = {ECG5000 Dataset (Time Series Classification Archive)},
  howpublished = {Verfügbar unter \url{http://timeseriesclassification.com/description.php?Dataset=ECG5000}},
  year         = {2025},
  note         = {Letzter Zugriff 2025‑06‑16}
}

@INPROCEEDINGS{iForest,
  author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  booktitle={2008 Eighth IEEE International Conference on Data Mining}, 
  title={Isolation Forest}, 
  year={2008},
  volume={},
  number={},
  pages={413-422},
  keywords={Application software;Credit cards;Detectors;Constraint optimization;Data mining;Information technology;Laboratories;Isolation technology;Performance evaluation;Astronomy;anomaly detection;outlier detection;novelty detection;isolation forest;binary trees;model based},
  doi={10.1109/ICDM.2008.17}}


@article{knn,
author = {Ramaswamy, Sridhar and Rastogi, Rajeev and Shim, Kyuseok},
title = {Efficient algorithms for mining outliers from large data sets},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/335191.335437},
doi = {10.1145/335191.335437},
abstract = {In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.},
journal = {SIGMOD Rec.},
month = may,
pages = {427–438},
numpages = {12}
}

@Inbook{minkowski,
author="{\c{S}}uhubi, Erdo{\u{g}}an S.",
title="Metric Spaces",
bookTitle="Functional Analysis",
year="2003",
publisher="Springer Netherlands",
address="Dordrecht",
pages="261--356",
abstract="This chapter is devoted to the study of a metric space in which a topology on a set X is generated by a non-negative real-valued scalar function called metric that may be interpreted as measuring some kind of a distance between any two elements, or points, of the set because some of its properties are quite reminiscent of the familiar notion of distance that we frequently encounter in daily life. This type of a topological space occupies a rather privileged position among all topological spaces because its topology is totally determined by a scalar distance function. We can safely presume that we are quite familiar with the properties of such a function and we are accustomed to deal effectively with it. Instead, a general topology is usually prescribed by some class of probably abstract subsets of an abstract set. The notion of a metric space was first introduced by Fr{\'e}chet in 1906. However, the term metric space was coined by Hausdorff a little later.",
isbn="978-94-017-0141-9",
doi="10.1007/978-94-017-0141-9_5",
url="https://doi.org/10.1007/978-94-017-0141-9_5"
}

@article{kauswahl,
  author       = {Upmanu Lall and Ashish Sharma},
  title        = {A nearest neighbor bootstrap for resampling hydrologic time series},
  journal      = {Water Resources Research},
  year         = {1996},
  volume       = {32},
  number       = {3},
  pages        = {679--693},
  doi          = {10.1029/95WR02966},
}

@ARTICLE{kauswahl2,
  author={Zhang, Shichao and Li, Xuelong and Zong, Ming and Zhu, Xiaofeng and Wang, Ruili},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Efficient kNN Classification With Different Numbers of Nearest Neighbors}, 
  year={2018},
  volume={29},
  number={5},
  pages={1774-1785},
  keywords={Training;Decision trees;Data mining;Predictive models;Learning systems;Technological innovation;Optics;Decision tree;k nearest neighbor (kNN) classification;sparse coding},
  doi={10.1109/TNNLS.2017.2673241}}

@article{randomProjection,
  author = {Paula Navarro-Esteban and Juan Antonio Cuesta-Albertos},
  title = {High-dimensional outlier detection using random projections},
  journal = {TEST},
  volume = {30},
  number = {4},
  pages = {908--934},
  year = {2021},
  doi = {10.1007/s11749-020-00750-y},
  url = {https://doi.org/10.1007/s11749-020-00750-y}
}

@book{mad,
  title={Volume 16: how to detect and handle outliers},
  author={Iglewicz, Boris and Hoaglin, David C},
  year={1993},
  publisher={Quality Press}
}

@article{mad2,
title = {Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median},
journal = {Journal of Experimental Social Psychology},
volume = {49},
number = {4},
pages = {764-766},
year = {2013},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2013.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0022103113000668},
author = {Christophe Leys and Christophe Ley and Olivier Klein and Philippe Bernard and Laurent Licata},
keywords = {Median absolute deviation, Outlier, MAD},
abstract = {A survey revealed that researchers still seem to encounter difficulties to cope with outliers. Detecting outliers by determining an interval spanning over the mean plus/minus three standard deviations remains a common practice. However, since both the mean and the standard deviation are particularly sensitive to outliers, this method is problematic. We highlight the disadvantages of this method and present the median absolute deviation, an alternative and more robust measure of dispersion that is easy to implement. We also explain the procedures for calculating this indicator in SPSS and R software.}
}

@article{konfusionsmatrix,
title = {An introduction to ROC analysis},
journal = {Pattern Recognition Letters},
volume = {27},
number = {8},
pages = {861-874},
year = {2006},
note = {ROC Analysis in Pattern Recognition},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2005.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S016786550500303X},
author = {Tom Fawcett},
keywords = {ROC analysis, Classifier evaluation, Evaluation metrics},
abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.}
}

@misc{fourierundwavelet,
  author       = {Stefan Krause},
  title        = {Fourier- und Wavelet-Transformation},
  howpublished = {Vorlesungsskript, htw Saar},
  year         = {2025},
  note         = {Unveröffentlichtes Lehrmaterial}
}